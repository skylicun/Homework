{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e64d1c",
   "metadata": {},
   "source": [
    "\n",
    "# LangGraph Reflection 机制开发指南\n",
    "\n",
    "本指南详细介绍了如何在 **LangGraph** 中构建基于大语言模型（LLM）的 **Reflection（反思）** 机制。\n",
    "\n",
    "Reflection 是一种重要的模型能力，通过让模型观察其过去的步骤和外部环境反馈，评估自身行为的质量，并不断改进输出。在生成与反思的循环中，模型可以逐步优化内容，从而提升生成质量和用户满意度。\n",
    "\n",
    "Reflection 机制被广泛应用于生成任务中，例如文章写作、内容修改与反馈、以及智能助理等场景。通过引导 LLM 进行自我反思和用户反馈处理，开发者可以让模型在多轮交互中自动调整其生成的内容，达到高效、精准、结构完善的输出。\n",
    "\n",
    "\n",
    "\n",
    "在本指南中，我们会逐步演示如何搭建这一机制，包括从基础的环境配置到生成器和反思器的构建，再到如何使用 LangGraph 状态图实现生成-反思循环的完整流程。无论您是为文章生成、内容评估，还是其他复杂任务设计 LLM 代理，本指南都将为您提供详细的开发思路和实用的代码示例。\n",
    "\n",
    "![reflection](./images/reflection.png)\n",
    "\n",
    "通过本指南，您将学习如何：\n",
    "1. 设置开发环境并安装所需包；\n",
    "2. 定义和生成灵活结构的文章，不局限于传统的五段式；\n",
    "3. 通过反思机制批改生成内容，并提供详细反馈；\n",
    "4. 构建反思与生成的状态循环，使模型持续改进生成内容。\n",
    "\n",
    "本开发指南适合任何希望构建复杂 LLM 任务的开发者，特别是需要实现生成-反思流程、文章批改反馈、或其他高级交互任务的场景。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e06a35-b8fb-4475-ac56-eef76a78e3b2",
   "metadata": {},
   "source": [
    "## 1. 环境设置\n",
    "首先，安装所需的包并设置API密钥："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d045265-8b0b-42e7-9bec-9e18e62a8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langgraph langchain-ollama tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c166149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# 定义一个帮助函数来检查环境变量，如果不存在则提示用户输入\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"请输入您的 {var}\")\n",
    "\n",
    "# 设置 OpenAI 和 Langchain API 密钥\n",
    "# _set_if_undefined(\"OPENAI_API_KEY\")\n",
    "# _set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "# _set_if_undefined(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec8159-c202-4274-b4cb-eddfa337940a",
   "metadata": {},
   "source": [
    "## 2. LangSmith开发配置\n",
    "LangSmith能够帮助您快速发现问题并提高LangGraph项目的性能。通过LangSmith，您可以使用跟踪数据来调试、测试和监控基于LangGraph构建的LLM应用程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c231a35a-8f08-44d1-abda-5d0defd00dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 LangSmith 中添加追踪功能\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Reflection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75943d-3e39-4765-811a-2c9a47cf3722",
   "metadata": {},
   "source": [
    "## 3. 定义写作助手智能体\n",
    "\n",
    "我们定义的这个助手是一个写作助手，旨在为用户生成高质量、结构清晰且引人入胜的文章。它的任务是根据用户的请求撰写内容，无论是短文、长篇、议论文还是其他类型的文章，都能够灵活应对。助手会专注于文章的清晰度、结构和质量，确保输出的内容是精心打磨过的。如果用户对生成的内容有反馈或建议，助手还能够根据这些反馈改进和优化文章，使其更符合用户的期望。这种互动机制保证了写作过程的灵活性和个性化，从而让用户获得更符合需求的成品。\n",
    "\n",
    "\n",
    "### System Prompt 详细解释：\n",
    "1. **\"You are a writing assistant\"**：写作助手的角色设定，让模型明确其任务是帮助用户进行写作。\n",
    "   \n",
    "2. **\"well-crafted, coherent, and engaging articles\"**：描述了文章应该具备的特性，包括“精心撰写的、连贯的和吸引人的”，但没有限制文章的具体结构，可以是不同类型的文章（如叙述文、议论文等）。\n",
    "\n",
    "3. **\"Focus on clarity, structure, and quality\"**：明确了撰写时需要关注的核心要素：清晰度、结构性和质量，确保输出内容优秀。\n",
    "\n",
    "4. **\"revise and improve the writing\"**：模型可以根据用户的反馈进行修改和优化，保持互动的灵活性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1905a06e-af05-4691-a6ed-014be2cfaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a reviewer tasked with providing constructive critique and improvement suggestions for the user's submission.\"\n",
    "            \" Offer detailed feedback, including recommendations on clarity, structure, content depth, and style, as well as areas for improvement.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0cec14-582a-4094-9a5b-9a0a2ae04a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = writer_prompt | ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q8_0\",\n",
    "    max_tokens=8192,\n",
    "    temperature=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a374db97-f61e-44d0-9fb7-8be1d3368a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下面是一个例子，展示了如何在Python中使用LangChain库加载Ollama大模型：\n",
      "\n",
      "```python\n",
      "import langchain\n",
      " import LLMModel, PromptTemplate\n",
      "\n",
      "模型。我们将使用Hugging Face Transformers中的Ollama。\n",
      " \"mrm8488/Ollama\"\n",
      "据你的机器配置选择合适的设备（GPU/TPU等）\n",
      "\n",
      "# 加载大模型\n",
      "llm_model = LLMModel(\n",
      ",   model_name\n",
      "    device=device,\n",
      ")\n",
      "\n",
      "我们创建更灵活的提示模板。板。PromptTemplate可以帮助\n",
      "变的用户输入。以定义一个可\n",
      "prompt_template = PromptTemplate(\n",
      "\"], input_variables=[\"user_input\n",
      "    template=\"请对{user_input}进行解释\",\n",
      ")\n",
      "\n",
      "一个LLM Chain，用于执行任务\n",
      ", prompt_template)Chain(llm_model\n",
      "\n",
      "例输入设为“Hello World！”\n",
      "Hello World!\")chain.run(user_input=\"\n",
      "\n",
      "print(task_result)\n",
      "```\n",
      "\n",
      "首先载入了一个大模型，然后定义了一个PromptTemplate，这是LangChain中的一个强大的提示模板构建块。最后，我们将该LLM Chain与一个输入值一起使用来产生输出。\n",
      "\n",
      "的是：你需要确保你的设备（GPU/TPU等）上有足够的可用资源来支持大模型的使用。同时，也要确保你已经正确安装了所需的库，包括LangChain和相关的Transformer库。你可以通过在命令行中运行pip install langchain transformers来进行此操作。"
     ]
    }
   ],
   "source": [
    "article = \"\"\n",
    "\n",
    "topic = HumanMessage(\n",
    "    content=\"帮我写一个基于langchain加载ollama大模型的python语句。\"\n",
    ")\n",
    "\n",
    "for chunk in writer.stream({\"messages\": [topic]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    article += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576b8163-56b5-4b49-9dd3-aaf14f1566db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "下面是一个例子，展示了如何在Python中使用LangChain库加载Ollama大模型：\n",
       "\n",
       "```python\n",
       "import langchain\n",
       "from langchain import LLMModel, PromptTemplate\n",
       "\n",
       "# 首先，我们需要定义我们的LLM模型。我们将使用Hugging Face Transformers中的Ollama。\n",
       "model_name = \"mrm8488/Ollama\"\n",
       "device = \"cuda\"  # 根据你的机器配置选择合适的设备（GPU/TPU等）\n",
       "\n",
       "# 加载大模型\n",
       "llm_model = LLMModel(\n",
       "    model_name,\n",
       "    device=device,\n",
       ")\n",
       "\n",
       "#定义我们的prompt模板。PromptTemplate可以帮助我们创建更灵活的提示模板。\n",
       "#例如，我们可以定义一个可变的用户输入。\n",
       "prompt_template = PromptTemplate(\n",
       "    input_variables=[\"user_input\"],\n",
       "    template=\"请对{user_input}进行解释\",\n",
       ")\n",
       "\n",
       "# 创建一个LLM Chain，用于执行任务\n",
       "chain = langchain.Chain(llm_model, prompt_template)\n",
       "\n",
       "# 使用这个链条完成任务。这里我们将示例输入设为“Hello World！”\n",
       "task_result = chain.run(user_input=\"Hello World!\")\n",
       "\n",
       "print(task_result)\n",
       "```\n",
       "\n",
       "在上面的示例中，我们首先载入了一个大模型，然后定义了一个PromptTemplate，这是LangChain中的一个强大的提示模板构建块。最后，我们将该LLM Chain与一个输入值一起使用来产生输出。\n",
       "\n",
       "然而，需要注意的是：你需要确保你的设备（GPU/TPU等）上有足够的可用资源来支持大模型的使用。同时，也要确保你已经正确安装了所需的库，包括LangChain和相关的Transformer库。你可以通过在命令行中运行pip install langchain transformers来进行此操作。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 使用Markdown显示优化后的格式\n",
    "display(Markdown(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa01c1-9074-41ae-810b-450edc7261ea",
   "metadata": {},
   "source": [
    "----------\n",
    "## 4. 定义审阅老师智能体\n",
    "\n",
    "下面我们使用反思机制批改生成的作文，生成一篇作文的反馈和建议。\n",
    "\n",
    "模型扮演“老师”角色，针对用户提交的作文进行打分、批改和提供改进建议。\n",
    "\n",
    "### System Prompt 详细解释：\n",
    "\n",
    "- **\"You are a teacher grading an essay submission.\"**\n",
    "  - 模型被设定为一个老师角色，专门负责为用户提交的作文进行批改。这一角色定位帮助模型理解其任务是提供具有建设性的反馈和评价。\n",
    "  \n",
    "- **\"Generate critique and recommendations for the user's submission.\"**\n",
    "  - 模型需要生成作文的批评与建议。它不只是评估作文的好坏，还需要指出需要改进的地方，并提出具体的建议。\n",
    "\n",
    "- **\"Provide detailed recommendations, including requests for length, depth, style, etc.\"**\n",
    "  - 这一部分进一步明确了反馈的细节，要求模型给出细致的建议。这包括：\n",
    "    - **Length（长度）**：文章的字数是否合适，是否需要扩展或删减。\n",
    "    - **Depth（深度）**：是否需要更深入的分析或讨论。\n",
    "    - **Style（风格）**：文章的写作风格是否合适，是否符合目标读者或主题的需求。\n",
    "  \n",
    "这一设定确保了模型不仅给出基本反馈，还可以根据文章的具体问题提出具体的改进意见，帮助用户更好地提升其写作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7001a65a-88ca-4ab7-bc66-fa1df870f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a teacher grading an article submission. writer critique and recommendations for the user's submission.\"\n",
    "            \" Provide detailed recommendations, including requests for length, depth, style, etc.\",\n",
    "\n",
    "        ),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a reviewer tasked with providing constructive critique and improvement suggestions for the user's submission.\"\n",
    "            \" Offer detailed feedback, including recommendations on clarity, structure, content depth, and style, as well as areas for improvement.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflect = reflection_prompt | ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q8_0\",\n",
    "    max_tokens=8192,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0c878a-f333-4fb6-b879-e7636d1087ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于你的要求，我为你提供一个基于langchain加载ollama大模型的python语句示例：\n",
      "\n",
      "```python\n",
      "import langchain\n",
      "Templategchain import LLMModel, Prompt\n",
      "import torch\n",
      "\n",
      "M模型。我们将使用Hugging Face Transformers中的Ollama。\n",
      "\"odel_name = \"mrm8488/Ollama\n",
      "() else \"cpu\")  # 根据你的机器配置选择合适的设备（GPU/TPU等）is_available\n",
      "\n",
      " 加载大模型\n",
      "llm_model = LLMModel(\n",
      "    model_name,\n",
      "    device=device,\n",
      ")\n",
      "\n",
      "prompt模板。PromptTemplate可以帮助我们创建更灵活的提示模板。\n",
      "变的用户输入。可以定义一个可\n",
      "prompt_template = PromptTemplate(\n",
      "_variables=[\"user_input\"],\n",
      "}进行解释\",plate=\"请对{user_input\n",
      ")\n",
      "\n",
      "用于执行任务LLM Chain，\n",
      " prompt_template).Chain(llm_model,\n",
      "\n",
      "将示例输入设为“Hello World！”\n",
      " = chain.run(user_input=\"Hello World!\")\n",
      "\n",
      "print(task_result)\n",
      "```\n",
      "\n",
      "**注意事项：**\n",
      "\n",
      "大模型之前，请确保你的设备（GPU/TPU等）上有足够的可用资源。\n",
      "  确保你已经正确安装了所需的库，包括LangChain和相关的Transformer库。你可以通过在命令行中运行`pip install langchain transformers`来进行此操作。\n",
      "\n",
      "**推荐改进：**\n",
      "\n",
      "使用大模型时，请确保你的设备配置足够高效，以避免资源占用过多。\n",
      " 如果你需要处理大量数据，考虑使用分布式计算或并行化技术来提高效率。\n",
      "你遇到任何问题或错误，请检查日志和错误信息，并尝试调试代码以解决问题。\n",
      "\n",
      "推荐阅读：**\n",
      "\n",
      "LangChain库的详细信息，包括使用指南、API参考和示例代码。\n",
      " Transformers文档：了解Hugging Face Transformers库的详细信息，包括模型列表、使用指南和API参考。\n",
      "\n",
      "**推荐资源：**\n",
      "\n",
      "：获取最新的LangChain库源码和更新信息。\n",
      "2.  Hugging Face Transformers GitHub仓库：获取最新的Hugging Face Transformers库源码和更新信息。"
     ]
    }
   ],
   "source": [
    "reflection = \"\"\n",
    "\n",
    "# 将主题（topic）和生成的文章（article）作为输入发送给反思智能体\n",
    "for chunk in reflect.stream({\"messages\": [topic, HumanMessage(content=article)]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c616014-c9c9-4d46-be9f-87485ee750eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "基于你的要求，我为你提供一个基于langchain加载ollama大模型的python语句示例：\n",
       "\n",
       "```python\n",
       "import langchain\n",
       "from langchain import LLMModel, PromptTemplate\n",
       "import torch\n",
       "\n",
       "# 首先，我们需要定义我们的LLM模型。我们将使用Hugging Face Transformers中的Ollama。\n",
       "model_name = \"mrm8488/Ollama\"\n",
       "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 根据你的机器配置选择合适的设备（GPU/TPU等）\n",
       "\n",
       "# 加载大模型\n",
       "llm_model = LLMModel(\n",
       "    model_name,\n",
       "    device=device,\n",
       ")\n",
       "\n",
       "# 定义我们的prompt模板。PromptTemplate可以帮助我们创建更灵活的提示模板。\n",
       "# 例如，我们可以定义一个可变的用户输入。\n",
       "prompt_template = PromptTemplate(\n",
       "    input_variables=[\"user_input\"],\n",
       "    template=\"请对{user_input}进行解释\",\n",
       ")\n",
       "\n",
       "# 创建一个LLM Chain，用于执行任务\n",
       "chain = langchain.Chain(llm_model, prompt_template)\n",
       "\n",
       "# 使用这个链条完成任务。这里我们将示例输入设为“Hello World！”\n",
       "task_result = chain.run(user_input=\"Hello World!\")\n",
       "\n",
       "print(task_result)\n",
       "```\n",
       "\n",
       "**注意事项：**\n",
       "\n",
       "1.  在使用大模型之前，请确保你的设备（GPU/TPU等）上有足够的可用资源。\n",
       "2.  确保你已经正确安装了所需的库，包括LangChain和相关的Transformer库。你可以通过在命令行中运行`pip install langchain transformers`来进行此操作。\n",
       "\n",
       "**推荐改进：**\n",
       "\n",
       "1.  在使用大模型时，请确保你的设备配置足够高效，以避免资源占用过多。\n",
       "2.  如果你需要处理大量数据，考虑使用分布式计算或并行化技术来提高效率。\n",
       "3.  如果你遇到任何问题或错误，请检查日志和错误信息，并尝试调试代码以解决问题。\n",
       "\n",
       "**推荐阅读：**\n",
       "\n",
       "1.  LangChain文档：了解LangChain库的详细信息，包括使用指南、API参考和示例代码。\n",
       "2.  Hugging Face Transformers文档：了解Hugging Face Transformers库的详细信息，包括模型列表、使用指南和API参考。\n",
       "\n",
       "**推荐资源：**\n",
       "\n",
       "1.  LangChain GitHub仓库：获取最新的LangChain库源码和更新信息。\n",
       "2.  Hugging Face Transformers GitHub仓库：获取最新的Hugging Face Transformers库源码和更新信息。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 使用Markdown显示优化后的格式\n",
    "display(Markdown(reflection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ad3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated  # 用于类型注解\n",
    "from langgraph.graph import END, StateGraph, START  # 导入状态图的相关常量和类\n",
    "from langgraph.graph.message import add_messages  # 用于在状态中处理消息\n",
    "from langgraph.checkpoint.memory import MemorySaver  # 内存保存机制，用于保存检查点\n",
    "from typing_extensions import TypedDict  # 用于定义带有键值对的字典类型\n",
    "\n",
    "# 定义状态类，使用TypedDict以保存消息\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # 使用注解确保消息列表使用add_messages方法处理\n",
    "\n",
    "# 异步生成节点函数：生成内容（如作文）\n",
    "# 输入状态，输出包含新生成消息的状态\n",
    "async def generation_node(state: State) -> State:\n",
    "    # 调用生成器(writer)，并将消息存储到新的状态中返回\n",
    "    return {\"messages\": [await writer.ainvoke(state['messages'])]}\n",
    "\n",
    "# 异步反思节点函数：对生成的内容进行反思和反馈\n",
    "# 输入状态，输出带有反思反馈的状态\n",
    "async def reflection_node(state: State) -> State:\n",
    "    # 创建一个消息类型映射，ai消息映射为HumanMessage，human消息映射为AIMessage\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    \n",
    "    # 处理消息，保持用户的原始请求（第一个消息），转换其余消息的类型\n",
    "    translated = [state['messages'][0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in state['messages'][1:]\n",
    "    ]\n",
    "    \n",
    "    # 调用反思器(reflect)，将转换后的消息传入，获取反思结果\n",
    "    res = await reflect.ainvoke(translated)\n",
    "    \n",
    "    # 返回新的状态，其中包含反思后的消息\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef78c4fb-2db3-45c0-9784-b73de4e7ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUND = 6\n",
    "\n",
    "# 定义条件函数，决定是否继续反思过程\n",
    "# 如果消息数量超过6条，则终止流程\n",
    "def should_continue(state: State):\n",
    "    if len(state[\"messages\"]) > MAX_ROUND:\n",
    "        return END  # 达到条件时，流程结束\n",
    "    return \"reflect\"  # 否则继续进入反思节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e188e5e-2327-4c78-927e-5f778fdca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建状态图，传入初始状态结构\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# 在状态图中添加\"writer\"节点，节点负责生成内容\n",
    "builder.add_node(\"writer\", generation_node)\n",
    "\n",
    "# 在状态图中添加\"reflect\"节点，节点负责生成反思反馈\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "\n",
    "# 定义起始状态到\"writer\"节点的边，从起点开始调用生成器\n",
    "builder.add_edge(START, \"writer\")\n",
    "\n",
    "\n",
    "# 在\"writer\"节点和\"reflect\"节点之间添加条件边\n",
    "# 判断是否需要继续反思，或者结束\n",
    "builder.add_conditional_edges(\"writer\", should_continue)\n",
    "\n",
    "# 添加从\"reflect\"节点回到\"writer\"节点的边，进行反复的生成-反思循环\n",
    "builder.add_edge(\"reflect\", \"writer\")\n",
    "\n",
    "# 创建内存保存机制，允许在流程中保存中间状态和检查点\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 编译状态图，使用检查点机制\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce358b-9b0d-4297-94e2-6ed8ab7e4dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28225cf-55bc-4cc3-8fc7-45064d224782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化图\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(\n",
    "            graph.get_graph(xray=True).draw_mermaid_png()\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error generating graph: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16cf4e0-abc5-4956-990c-09f78254b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 定义装饰器，记录函数调用次数\n",
    "def track_steps(func):\n",
    "    step_counter = {'count': 0}  # 用于记录调用次数\n",
    "    \n",
    "    def wrapper(event, *args, **kwargs):\n",
    "        # 增加调用次数\n",
    "        step_counter['count'] += 1\n",
    "        # 在函数调用之前打印 step\n",
    "        display(Markdown(f\"## Round {step_counter['count']}\"))\n",
    "        # 调用原始函数\n",
    "        return func(event, *args, **kwargs)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# 使用装饰器装饰 pretty_print_event_markdown 函数\n",
    "@track_steps\n",
    "def pretty_print_event_markdown(event):\n",
    "    # 如果是生成写作部分\n",
    "    if 'writer' in event:\n",
    "        generate_md = \"#### 写作生成:\\n\"\n",
    "        for message in event['writer']['messages']:\n",
    "            generate_md += f\"- {message.content}\\n\"\n",
    "        display(Markdown(generate_md))\n",
    "    \n",
    "    # 如果是反思评论部分\n",
    "    if 'reflect' in event:\n",
    "        reflect_md = \"#### 评论反思:\\n\"\n",
    "        for message in event['reflect']['messages']:\n",
    "            reflect_md += f\"- {message.content}\\n\"\n",
    "        display(Markdown(reflect_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64544540-9594-4812-a66a-c019284bdf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2954d151-db4c-46cf-97db-1ce5bf9fc7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Round 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 写作生成:\n",
       "- Sure! 以下是一个示例Python代码片段，基于LangChain加载Ollama大模型：\n",
       "```python\n",
       "import langchain\n",
       "\n",
       "# 定义语言链模型\n",
       "model = langchain.LLM(\n",
       "    model_name=\"ollama-base\",\n",
       "    temperature=0.7,\n",
       ")\n",
       "\n",
       "# 创建一个LLM Chatbot实例\n",
       "chatbot = langchain.Chatbot(model=model, max_length=100)\n",
       "\n",
       "# 使用Chatbot进行对话\n",
       "def talk(input_text):\n",
       "    return chatbot.get_response(input_text)\n",
       "\n",
       "# 测试聊天\n",
       "input_text = \"你好！我想问一些问题。\"\n",
       "output_text = talk(input_text)\n",
       "print(output_text)\n",
       "```\n",
       "这个代码片段定义了一个基于LangChain的语言链模型，使用Ollama大模型进行对话。`LLM`类表示一个模型对象，传入参数 `model_name=\"ollama-base\"` 指定了使用哪个模型。`temperature=0.7` 是控制生成输出文本的温度参数，可以调节输出文本的随机性。\n",
       "\n",
       "`Chatbot` 类创建一个基于语言链模型的聊天机器人实例，`max_length=100` 指定最大输入长度。然后定义了一个名为 `talk()` 的函数，使用这个Chatbot实例对用户输入进行回复。\n",
       "\n",
       "最后，用 `print(output_text)` 输出 Chatbot 的回复结果。\n",
       "\n",
       "注意：在实际运行代码前，请确保您已经安装LangChain和相关模型的依赖库，以及您的环境配置正确。\n",
       "\n",
       "也许会有几个疑问\n",
       "\n",
       "1.  我应该如何将此示例与我的具体任务或应用集成？\n",
       "2.  如何调整超参数（如温度、最大长度等）来改善对话质量？\n",
       "3.  有什么方法可以进一步扩展和增强这段代码以适应更复杂的任务或需求？\n",
       "\n",
       "您也许会有更多问题，但如果能回答这些基本疑问，我们可能能给出个更详细的回复。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 基于LangChain加载Ollama大模型的Python语句示例已经提供了一个基础框架，可以用于创建一个简单的聊天机器人。以下是对该示例的进一步分析和建议：\n",
       "\n",
       "**1. 集成到具体任务或应用**\n",
       "\n",
       "要将此示例与您的具体任务或应用集成，您可以尝试以下几点：\n",
       "\n",
       "*   **定义自定义输入处理函数**: 根据您的需求，创建一个函数来处理用户输入，例如文本预处理、分词等。\n",
       "*   **扩展Chatbot功能**: 添加更多的功能，如回答特定问题、提供信息、生成内容等。\n",
       "*   **集成到现有系统**: 将LangChain和Ollama模型整合到您的应用或系统中，例如使用API接口或消息队列。\n",
       "\n",
       "**2. 调整超参数**\n",
       "\n",
       "为了调整超参数（如温度、最大长度等）来改善对话质量，您可以尝试以下几点：\n",
       "\n",
       "*   **实验不同温度值**: 通过改变`temperature`参数值来观察对输出文本的影响。\n",
       "*   **调整最大长度**: 根据您的需求，修改`max_length`参数值以控制Chatbot回复的长度。\n",
       "*   **使用其他超参数**: LangChain和Ollama模型可能提供更多可调超参数，您可以尝试调整这些参数来找到最佳设置。\n",
       "\n",
       "**3. 扩展和增强代码**\n",
       "\n",
       "要进一步扩展和增强这段代码以适应更复杂的任务或需求，您可以尝试以下几点：\n",
       "\n",
       "*   **使用多模型**: 将LangChain与其他语言模型（如BERT、RoBERTa等）结合使用，以实现更好的性能。\n",
       "*   **集成到机器学习框架**: 使用深度学习库（如TensorFlow、PyTorch等）来训练和优化Chatbot模型。\n",
       "*   **添加多模态支持**: 将语言链模型与图像或音频处理结合使用，以实现多模态对话。\n",
       "\n",
       "总之，这个示例提供了一个基础框架，可以作为您探索LangChain和Ollama大模型的起点。通过调整超参数、扩展功能以及集成到您的应用中，您可以创建更复杂且高效的聊天机器人。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 写作生成:\n",
       "- 要进一步改进代码，可以考虑以下优化：\n",
       "\n",
       "1.  **错误处理和异常管理**：为输入处理函数、LLM Chatbot实例和相关方法增加错误处理以确保程序对不良输入或其他意外情况有更好的响应。\n",
       "2.  **文本预处理和分词**：在用户输入的文本中进行语言分析，如词性标注、名词短语提取等，以提供更具体而相关的信息给LLM Chatbot，帮助它生成更准确且有意义的回复。\n",
       "3.  **参数可视化和调整**：使用可视化工具来图形化超参数（如温度、最大长度）与Chatbot表现之间的关系，这可以大大简化调优过程并提高对话质量。\n",
       "4.  **模型更新和训练**：实现基于经验学习的模型改进方法，将Chatbot性能的数据用于更新和重新训练LLM Chatbot，以持续提升其语言理解和生成能力。\n",
       "\n",
       "您可以结合 LangChain 库与其他机器学习技术（例如使用 PyTorch 或 TensorFlow 等深度学习库）来进一步增强该示例。同时，请记住考虑到实际应用的负载、成本等因素进行优化以实现最佳结果。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 基于LangChain加载Ollama大模型的Python语句示例已经提供了一个基础框架，可以用于创建一个简单的聊天机器人。以下是对该示例的进一步分析和建议：\n",
       "\n",
       "**1. 错误处理和异常管理**\n",
       "\n",
       "要为输入处理函数、LLM Chatbot实例和相关方法增加错误处理，您可以尝试以下几点：\n",
       "\n",
       "*   **定义自定义错误类**: 根据您的需求，创建一个自定义错误类来捕获特定类型的错误。\n",
       "*   **使用try-except块**: 在关键代码区域使用try-except块来捕获异常并提供友好的错误信息。\n",
       "*   **日志记录**: 使用日志库（如Loguru或Python标准库中的logging）来记录程序运行过程中出现的错误和警告。\n",
       "\n",
       "**2. 文本预处理和分词**\n",
       "\n",
       "要在用户输入的文本中进行语言分析，您可以尝试以下几点：\n",
       "\n",
       "*   **使用NLTK或spaCy**: 将自然语言处理（NLP）库（如NLTK或spaCy）集成到您的程序中，以提供基本的文本预处理功能，如分词、词性标注等。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来进行特定类型的文本分析，如名词短语提取等。\n",
       "\n",
       "**3. 参数可视化和调整**\n",
       "\n",
       "要使用可视化工具来图形化超参数与Chatbot表现之间的关系，您可以尝试以下几点：\n",
       "\n",
       "*   **使用Matplotlib或Seaborn**: 将数据可视化库（如Matplotlib或Seaborn）集成到您的程序中，以创建交互式图表。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来生成特定类型的图表，如散点图、条形图等。\n",
       "\n",
       "**4. 模型更新和训练**\n",
       "\n",
       "要实现基于经验学习的模型改进方法，您可以尝试以下几点：\n",
       "\n",
       "*   **使用PyTorch或TensorFlow**: 将深度学习库（如PyTorch或TensorFlow）集成到您的程序中，以提供模型训练和优化功能。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来更新和重新训练LLM Chatbot。\n",
       "\n",
       "总之，这个示例提供了一个基础框架，可以作为您探索LangChain和Ollama大模型的起点。通过调整超参数、扩展功能以及集成到您的应用中，您可以创建更复杂且高效的聊天机器人。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 写作生成:\n",
       "- 上述步骤已经给出了改进代码和程序结构方面的指导。基于这些建议和考虑多种技术和方法的组合，可以进一步完善该示例。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 评论反思:\n",
       "- 基于LangChain加载Ollama大模型的Python语句示例已经提供了一个基础框架，可以用于创建一个简单的聊天机器人。以下是对该示例的进一步分析和建议：\n",
       "\n",
       "**1. 错误处理和异常管理**\n",
       "\n",
       "要为输入处理函数、LLM Chatbot实例和相关方法增加错误处理，您可以尝试以下几点：\n",
       "\n",
       "*   **定义自定义错误类**: 根据您的需求，创建一个自定义错误类来捕获特定类型的错误。\n",
       "*   **使用try-except块**: 在关键代码区域使用try-except块来捕获异常并提供友好的错误信息。\n",
       "*   **日志记录**: 使用日志库（如Loguru或Python标准库中的logging）来记录程序运行过程中出现的错误和警告。\n",
       "\n",
       "**2. 文本预处理和分词**\n",
       "\n",
       "要在用户输入的文本中进行语言分析，您可以尝试以下几点：\n",
       "\n",
       "*   **使用NLTK或spaCy**: 将自然语言处理（NLP）库（如NLTK或spaCy）集成到您的程序中，以提供基本的文本预处理功能，如分词、词性标注等。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来进行特定类型的文本分析，如名词短语提取等。\n",
       "\n",
       "**3. 参数可视化和调整**\n",
       "\n",
       "要使用可视化工具来图形化超参数与Chatbot表现之间的关系，您可以尝试以下几点：\n",
       "\n",
       "*   **使用Matplotlib或Seaborn**: 将数据可视化库（如Matplotlib或Seaborn）集成到您的程序中，以创建交互式图表。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来生成特定类型的图表，如散点图、条形图等。\n",
       "\n",
       "**4. 模型更新和训练**\n",
       "\n",
       "要实现基于经验学习的模型改进方法，您可以尝试以下几点：\n",
       "\n",
       "*   **使用PyTorch或TensorFlow**: 将深度学习库（如PyTorch或TensorFlow）集成到您的程序中，以提供模型训练和优化功能。\n",
       "*   **定义自定义函数**: 根据您的需求，创建一个自定义函数来更新和重新训练LLM Chatbot。\n",
       "\n",
       "总之，这个示例提供了一个基础框架，可以作为您探索LangChain和Ollama大模型的起点。通过调整超参数、扩展功能以及集成到您的应用中，您可以创建更复杂且高效的聊天机器人。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Round 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 写作生成:\n",
       "- 该方案已经给出了关于优化和完善基础框架以进行更复杂和高效的项目开发的具体指导，包括实现基本的错误处理、文本分析以及可视化函数。根据该方案，可以继续完善语言分析模块，并考虑多种技术和方法的组合以提高程序性能和可靠性。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"帮我写一个基于langchain加载ollama大模型的python语句。\")\n",
    "    ],\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(inputs, config):\n",
    "    pretty_print_event_markdown(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9e339-eb87-4707-a019-8387faaa52bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44173cab-4e51-4bd7-8c97-a28748e159a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f693e-441e-4536-8459-8042b34e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6092e5ee-7263-4f62-818a-0cc33ca0b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"参考西游记唐僧的说话风格，写一篇劝年轻人结婚买房的文章\")\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# async for event in graph.astream(inputs, config):\n",
    "#     pretty_print_event_markdown(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e9c24-7cf7-42fa-be13-fbfebfdad5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a5187b-6b3b-4266-a226-f1e56e5ad350",
   "metadata": {},
   "source": [
    "## Homework: \n",
    "\n",
    "1. 扩展本指南的 Reflection Agent，使其能够完成更通用的生成任务，包括但不限于代码、报告等；\n",
    "2. 使用扩展后的 Reflection Agent 生成代码，实现在 GitHubSentinel 上新增一个信息渠道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b745d-ce5b-4a90-a3e8-64ecb3499c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affbb4d-f154-43e6-ae3d-d6a5dcd6e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f9e0b07-62a8-430b-bf01-b7d8c07dd6e5",
   "metadata": {},
   "source": [
    "### 如何让 Reflection `System Prompt` 更加通用：\n",
    "\n",
    "如果你想让这个 `System Prompt` 适用于更广泛的内容评估场景，不局限于作文，你可以做一些轻微的调整。例如：\n",
    "\n",
    "```python\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a reviewer tasked with providing constructive critique and improvement suggestions for the user's submission.\"\n",
    "            \" Offer detailed feedback, including recommendations on clarity, structure, content depth, and style, as well as areas for improvement.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### 修改后的变化：\n",
    "1. **角色定位更广泛**：从“老师”改为“审阅者”，这样不局限于评估作文，适用于各种类型的内容，包括文章、报告、甚至代码审查。\n",
    "  \n",
    "2. **批评与改进建议的灵活性**：从作文的“长度、深度、风格”拓展为“清晰度、结构、内容深度、风格”，这使得反馈更加多样化，适用于不同的内容类型。\n",
    "\n",
    "通过这种方式，可以让模型在更多场景下提供高质量的评估和反馈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b78d64-791c-484c-922d-d00a01b78a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
