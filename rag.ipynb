{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "439df52a-dadf-4fd8-a0ad-e59bac7ca529",
   "metadata": {},
   "source": [
    "# 使用 LangChain 构建一个 RAG 应用\n",
    "\n",
    "## RAG 是什么\n",
    "\n",
    "RAG 是一种将检索到的文档上下文与大语言模型（LLM）结合起来生成答案的技术。\n",
    "\n",
    "整个过程主要分为以下几个步骤：\n",
    "\n",
    "1. 加载文档：将原始数据(来源可能是在线网站、本地文件、各类平台等)加载到 LangChain 中。\n",
    "1. 文档分割：将加载的文档分割成较小的块，以适应模型的上下文窗口，并更容易进行向量嵌入和检索。\n",
    "1. 存储嵌入：将分割后的文档内容嵌入到向量空间，并存储到向量数据库中，以便后续检索。\n",
    "1. 检索文档：通过查询向量数据库，检索与问题最相关的文档片段。\n",
    "1. 生成回答：将检索到的文档片段与用户问题组合，生成并返回答案。\n",
    "\n",
    "通过这些步骤，可以构建一个强大的问答系统，将复杂任务分解为更小的步骤并生成详细回答。\n",
    "\n",
    "![rag](../images/rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d81c83-4083-4ec1-ac4c-e96840024278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a5b694-e332-4311-a20f-4e55fc9e3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain_community in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081d8fd0-0718-43ab-bc96-1a7cde8569b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-chroma\n",
      "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-chroma) (0.115.6)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-chroma) (0.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-chroma) (1.26.4)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.10.4)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.34.0)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading grpcio-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.15.1)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.9.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.41.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (0.1.147)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.7.0)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.4,>=0.1.40->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Requirement already satisfied: protobuf in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.29.2)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.3)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading wrapt-1.17.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (11.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=775d19484f81938711c0b1e43dd1f66da119ab07fb8dc2b39632e758cf23fd68\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, wrapt, uvloop, python-dotenv, pyproject_hooks, pyasn1, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, httptools, grpcio, googleapis-common-protos, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, onnxruntime, deprecated, build, opentelemetry-api, google-auth, opentelemetry-semantic-conventions, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.23 deprecated-1.2.15 durationpy-0.9 flatbuffers-24.12.23 google-auth-2.37.0 googleapis-common-protos-1.66.0 grpcio-1.68.1 httptools-0.6.4 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 posthog-3.7.4 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 requests-oauthlib-2.0.0 rsa-4.9 uvloop-0.21.0 watchfiles-1.0.3 wrapt-1.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain-chroma -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b363d-c56e-420e-b8c1-76420b6cfa46",
   "metadata": {},
   "source": [
    "## **RAG 开发指南**\n",
    "\n",
    "**本指南将详细介绍如何使用 LangChain 框架构建一个基于检索增强生成 (RAG) 的应用。**\n",
    "\n",
    "下面是基于 LangChain 实现的 RAG 的核心步骤与使用到的关键代码抽象（类型、方法、库等）:\n",
    "\n",
    "1. **加载文档**: 使用 `WebBaseLoader` 类从指定来源加载内容，并生成 `Document` 对象（依赖 `bs4` 库）。\n",
    "2. **文档分割**: 使用 `RecursiveCharacterTextSplitter` 类的 `split_documents()` 方法将长文档分割成较小的块。\n",
    "3. **存储嵌入**: 使用 `Chroma` 类的 `from_documents()` 方法将分割后的文档内容嵌入向量空间，并存储在向量数据库中（使用 `OpenAIEmbeddings`），并可以通过检查存储的向量数量来确认存储成功。。\n",
    "4. **检索文档**: 使用 `VectorStoreRetriever` 类的 `as_retriever()` 和 `invoke()` 方法基于查询从向量数据库中检索最相关的文档片段。\n",
    "5. **生成回答**: 使用 `ChatOpenAI` 类的 `invoke()` 方法，将检索到的文档片段与用户问题结合，生成回答（通过 `RunnablePassthrough` 和 `StrOutputParser`）。\n",
    "\n",
    "我们使用的文档是Lilian Weng撰写的《LLM Powered Autonomous Agents》博客文章（https://lilianweng.github.io/posts/2023-06-23-agent/ ），最终构建好的 RAG 应用支持我们询问关于该文章内容的相关问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5318a1-2bb9-45bb-8206-82fcf024c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd3284-1cd1-4f53-b105-7f4f5d70faf3",
   "metadata": {},
   "source": [
    "### Step 1: 加载文档\n",
    "\n",
    "- **描述**: 使用 `DocumentLoader` 从指定来源（如网页）加载内容，并将其转换为 `Document` 对象。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `WebBaseLoader`\n",
    "  - 方法: `load()`\n",
    "  - 库: `bs4` (BeautifulSoup)\n",
    "- **代码解释**:\n",
    "  - **文档加载**: 使用 `WebBaseLoader` 从网页加载内容，并通过 `BeautifulSoup` 解析 HTML，提取重要的部分。\n",
    "  - **检查加载数量**: 打印加载的文档数量，确保所有文档正确加载。\n",
    "  - **验证文档内容**: 输出第一个文档的部分内容，确认加载的数据符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ccef98-31df-4a7d-a5d1-94785651e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 WebBaseLoader 从网页加载内容，并仅保留标题、标题头和文章内容\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"postTitle\",\"postBody\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.cnblogs.com/JavaEdge/p/18559650\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbefad84-e2df-4984-9583-6ebe4450f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\n\\nLLM部署，你必须要知道的几个技巧！\\n\\n\\n\\n0 前言\\n今天我会首先解释为什么 LLM 的部署很难，因为许多人可能并不理解其中的复杂性。接着，我会分享七个提高 LLM 部署效果的技巧和方法。\\n1 为啥 LLM 部署困难？\\n“最近在忙啥？”\\n“我一直在让 LLM 服务变得更简单。”\\n“LLM 部署难吗？不是直接调用 OpenAI API 就行？”\\n“某种程度上是这样。”因为提到 LLM，大多数人只会想到 OpenAI，调用 API 确实简单。她为什么要谈这些内容？调用 API 谁不会？但实际上，访问 LLM 的方式不止一种。可用托管的API如 OpenAI、Cohere、Anthropic 和 AI21 Labs 等。他们已为你完成托管和部署，你只需调它们。虽然这确实减少你的工作量，但仍存在复杂性，如减少幻觉输出。不过，他们已经完成很多繁重任务。很多场景，你可能更倾向自托管，如调用 Mistral或托管 Llama 或其他模型。这意味着你在自己的环境中托管它，无论VPC还是PC。\\n那为啥还自托管？\\n很多原因：\\n\\n降低大规模部署成本。如只做概念验证，基于 OpenAI API 模型成本确实低。但如大规模部署，自托管最终成本更低。因为只需解决自己的业务问题，可用更小模型，而 OpenAI 必须托管一个能解决编程和写作莎士比亚问题的大模型，因此需要更大的模型。大规模部署时，自托管成本会低得多\\n性能提升。当你用特定任务的LLM或对其微调，使其专注你的任务，通常得到更好性能\\n大多数客户选择自托管的原因：隐私和安全。如你处受监管行业，如需遵循 GDPR 或满足合规团队的要求，你可能也需自托管\\n\\n如果这几点不重要，就用 API 够了。\\n企业选择开源的主要原因\\n包括控制权、定制化和成本。最重要的是控制权。拥有 AI 独立性至关重要，如当 OpenAI 再次解雇 CEO，你仍可访问自己的模型，尤其是当你构建重要的业务应用时。如果你正在考虑自托管，你绝对不是孤军奋战，大多数企业都在努力建立自托管能力。\\n对冲基金的一员说：“隐私对我的用例很重要，因此自托管是有意义的。”然后他可能会问：“自托管真的有那么难吗？”我经常听到类似的话，这让我非常恼火。答案是：确实更难。你不能忽视那些你看不到的复杂性。当你调用基于 API 的模型时，你受益于他们的工程师在构建推理和服务基础设施方面所做的所有努力。实际上，像 OpenAI 这样的公司有 50 到 100 人的团队在管理这些基础设施。包括模型压缩、Kubernetes、批处理服务器、函数调用、JSON 生成、运行时引擎等。当你使用 API 模型时，这些你都不需要操心，但当你自托管时，这些问题突然变成了你的责任。\\n他可能会说：“但我经常部署机器学习模型，比如 XGBoost 或线性回归模型。部署这些 LLM 会有多难？”我们的回答是：“你知道 L 代表什么吗？”部署这些模型要困难得多。为什么呢？LLM 中的第一个 L 代表“大”（Large）。我记得我们刚成立公司时，认为一个拥有 1 亿参数的 BERT 模型已经算大了。现在，一个拥有 70 亿参数的模型被认为是小型模型，但它仍然有 14GB 的大小，这绝对不小。\\n第二个原因是 GPU。与 CPU 相比，GPU 更难处理，它们也更昂贵，因此高效利用 GPU 十分重要。如果你对 CPU 的利用率不高，可能问题不大，因为它们成本低得多。但对于 GPU，成本、延迟和性能之间的权衡非常明显，这是以前可能没有遇到过的。\\n第三个原因是，这个领域发展非常快。我们现在用于部署、优化和服务模型的技术，有一半在一年前还不存在。还有一个值得一提的问题是编排问题。通常，对于这些大语言模型应用，你需要协调多个不同的模型。例如，RAG（检索增强生成）就是一个典型的例子。你需要协调一个嵌入模型和一个生成模型。如果是最先进的 RAG，你可能还需要多个解析模型，比如图像模型和表格模型，此外还需要一个重排序模型。最终，你可能会用到五六个不同的模型。这会让人感到非常困惑。此外，部署应用还有其他常见难点，比如扩展性和可观察性。\\n2 咋让 LLM 部署更轻松？\\n分享一些让 LLM 部署更轻松的技巧。虽然仍会很痛苦，但不会那么糟糕。\\n1. 知道你的部署边界\\n构建应用程序时，应解你的部署边界。通常，人们在构建出一个自认为可行的应用程序后，才开始考虑部署边界。我认为，你应该先花时间思考你的需求，这会让后续一切变得更简单。如考虑你的：\\n\\n延迟需求是什么？\\n预计负载是多少？\\n应用程序是顶多只有三个用户，还是像 DoorDash 一样要服务数百万用户？\\n有什么硬件资源可用？\\n需要在本地部署，还是可用云实例？如是云实例，需要什么类型实例？\\n\\n所有这些问题都要提前规划。你可能无法知道精确需求，所以最好列出范围。如：“只要延迟低于 1 秒就可以接受。”或“只要高于某个值也行。”。还有一些问题如：我是否需要保证输出是 JSON 格式？是否需要保证输出符合特定的正则表达式规则？这些都值得提前思考。\\n2. 始终进行量化\\n提前规划好这些需求，那后续所有决策都容易得多。始终对模型进行量化。量化本质是一种模型压缩技术，它将LLM的权重精度降低到你想要的任何形式。4-bit 是我最喜欢的量化，从 FP32（32位浮点数）开始。因为它在准确性和压缩比之间达到极佳平衡。你可以看到这张图表，我们有一个准确性与模型位数的关系图，也就是模型的大小。\\n假设原始模型是 FP16（16位浮点数），其实它通常是 32 位的。红线表示它的准确性。当我们压缩模型时，比如从 FP16 降低到 4-bit，固定资源下，使用量化模型的性能实际上要好于未量化的模型。通过这张图表我们可以得出结论，对于固定资源，量化模型通常能够在准确性和资源利用率之间取得更好的平衡。\\n我们从基础设施开始，倒推需求。假设我们可用 L40S GPU，它有 48GB 显存。因为我们知道可用的资源，可以根据现有的模型倒推需求。如是 Llama 13B（130亿参数）模型，它需要 26GB 显存，没问题，可运行。但如是当前最先进 Mixtral 模型，它无法直接运行。然而，一个经 4-bit 量化的 Mixtral 模型可运行，这就很棒了。通过这种方式，就知道哪些模型可用来实验。\\n那个关于 Tim Dettmers 的图表也告诉我们，4-bit 量化模型在性能上可能更优。假设 Llama 模型和 Mixtral 模型体积一样，4-bit 模型通常会保留原来模型的高精度，同时大大减小了模型体积。我们通过基础设施倒推，找到能适配资源的量化模型，这很可能是当前性能最优的解决方案。\\n3. 花时间优化推理\\n建议只花一点时间是因为，部署这些模型时，你最初想到的策略往往是完全错误的。虽然你不需要花大量时间思考这个问题，但稍微投入一些时间，可以使 GPU 利用率提升几个数量级。\\n举个例子，关于批处理策略。批处理是指多个请求同时处理。部署这些模型时，GPU 利用率是最宝贵的资源。因为 GPU 很昂贵，所以最大化其利用率非常重要。\\n如果我不使用批处理，那么 GPU 的利用率大概是这样的，非常糟糕。一个常见的错误做法是使用动态批处理，这种方法适用于非生成式 AI 应用，比如你之前可能用过的系统。动态批处理的原理是等待一小段时间，收集在这段时间内到达的请求，然后一起处理。在生成式模型中，这种方法会导致 GPU 利用率下降。开始时利用率很高，但随后会下降，因为用户会因较长的生成时间被卡在队列中。\\n动态批处理虽然是常见做法，但通常效果不好。如果你花点时间思考这个问题，可以采用持续批处理（Continuous Batching）。这是我们使用的一种方法，也是当前生成式模型的最先进批处理技术。它允许新到的请求中断正在处理的请求，以保持 GPU 利用率始终处于高水平。这样不仅减少了排队时间，还大幅提升了资源利用效率。这张 GPU 利用率图表是我们几周前的状态。相比动态批处理，持续批处理在 GPU 成本上可以带来一个数量级的提升。这完全不影响模型准确性，但大大提高了利用率。\\n对于非常大的模型，单个 GPU 无法满足推理需求。例如，Llama 70B、Mixtral 或 Jamba 等模型非常庞大。通常需要将它们分布在多个 GPU 上进行推理。这要求你能够设计一种多 GPU 推理的方法。最常见的方法（例如 Hugging Face 的 Accelerate 推理库所使用的方式）是按层级划分模型。如果模型占用 90GB，可以分配 30GB 给每个 GPU，共使用三个 GPU。然而，这种方法的缺点是每次只有一个 GPU 处于活跃状态，导致资源浪费，因为后续 GPU 需要等待前一个 GPU 完成任务。\\n这种方式存在局限性，例如在 Hugging Face Accelerate 库中。我们认为更优的方法是 Tensor Parallel。这种方式将模型按“长度”分割，使每个 GPU 能同时运行每一层，从而大幅提升推理速度，并支持任意大小的模型。所有 GPU 同时运行，因此避免了资源浪费。例如，在一个模型中，可以实现 GPU 利用率提升 3 倍，再加上其他优化，可以显著提升资源效率。\\n\\n4. 整合基础设施\\n目前为止，我的建议包括：考虑部署需求、量化、推理优化。第四个建议是整合基础设施。生成式 AI 的计算成本非常高，因此集中的基础设施管理能带来很大优势。传统企业的机器学习团队往往以孤岛形式存在，导致基础设施整合效率低下。通过集中的 MLOps 团队（如 Ian 所领导的团队），可实现一次性部署并由单一团队进行维护，这让应用开发团队专注于构建应用。\\n举个例子，一个中央计算基础设施可以提供访问模型（如 Llama 70、Mixtral 和 Gemma 7B）的权限，并由中央团队定期更新模型（例如从 Llama 2 升级到 Llama 7）。各个应用开发团队可以个性化模型，例如添加 LoRA（轻量化适配器）或 RAG（结合专有数据的检索增强生成）。中央团队负责维护基础设施，而分散的开发团队仅需调用这些模型构建应用。这种方法不仅提高了 GPU 的利用率，还为组织提供类似 OpenAI 的体验，但使用的是私有模型。\\n关键点包括：确保推理服务器具备可扩展性、支持 LoRA 适配器以实现微调。如果做好这些工作，可以显著提升 GPU 利用率。GPU 的利用率非常重要，甚至可以说是仅次于家人和朋友的存在。\\n\\n案例研究：RNL\\n一个美国企业 RNL 拥有四个不同的生成式 AI 应用，每个应用使用独立 GPU。这种方式导致了 GPU 利用率低下，因为不是所有应用始终满负荷运行。我们帮助他们将所有应用资源整合到一个推理服务器中，使各团队通过共享资源构建应用。这种方式将所需 GPU 数量减少了一半，同时也能更高效地管理生成式和非生成式任务。\\n\\n5. 构建时考虑模型替换周期\\n建议的第五点是，假设在 12 个月内需要替换模型。随着 LLM 的快速发展，仅通过切换模型即可获得性能提升。例如，一个客户去年使用 Llama 1 开发了首个应用程序，在一年内更换了四次模型。\\n每周他们都会说，这个新模型出来了。你们支持吗？我会说，是的，但为什么这是第六次更改了？让我们回想一下一年前最先进的技术是什么。一年前，也许那时Llama已经发布了，但如果在那之前，可能是T5系列。T5模型是当时最好的开源模型。我们所见证的是开源大语言模型生态系统的惊人爆发。这一切都始于Llama，然后是Llama 2，接着许多企业在此基础上构建。\\n例如，Mistral 70B实际上是用与Llama相同的架构构建的。我们有来自阿联酋的Falcon。我们有Mistral的Mixtral。你们有很多，而且它们还在不断涌现。实际上，如果你查看Hugging Face，这是所有这些模型存储的地方，如果你查看他们的开源模型排行榜，顶级模型几乎每周都在变化。最新和最伟大的模型不断出现。这些模型将会不断变得更好。这是所有模型的性能，无论是开源还是非开源，你可以看到许可证，专有的或非专有的。开源模型正在慢慢地占据排行榜。我们开始接近开源和非开源之间的平等。现在，开源模型大约在GPT-3.5左右。那是我们所有人都为之惊叹的原始ChatGPT。\\n我的预期是，我们将在未来一年内达到GPT-4的质量。这意味着你真的不应该将自己与单一模型或单一供应商绑定。回到我之前向你们展示的a16z报告，大多数企业都在使用多个模型供应商。他们正在以一种可互操作的方式构建他们的推理栈，如果OpenAI出现故障，我可以轻松地将其替换为Llama模型。或者，如果现在Claude比GPT-4更好，我可以很容易地替换它们。以这种可互操作性为念进行构建真的很重要。我认为OpenAI给我们的最伟大的事情不一定是他们的模型，尽管它们真的很棒，但他们实际上违反直觉地民主化了AI领域，不是因为他们开源了他们的模型，因为他们真的没有，而是因为他们为行业提供了API的统一性。如果你以OpenAI API为念进行构建，那么你就可以捕捉到很多价值，并且能够轻松地替换模型。\\n这对构建方式意味着什么？以API和容器为先的开发使生活变得更轻松。这是相当标准的事情。抽象真的很好，所以不要花时间为你的特定模型构建自定义基础设施。你很可能在12个月内不会使用它。如果你要构建，尝试构建更通用的基础设施。我们总是说，在当前阶段，我们仍在许多组织中证明AI的价值，工程师应该花时间构建出色的应用体验，而不是纠结于基础设施。因为现在，对于大多数企业来说，我们很幸运有足够的预算去尝试这些生成式AI的东西。\\n我们需要快速证明价值。我们倾向于说，不要使用只支持Llama的框架，因为这只会给你带来更多麻烦。无论你选择什么架构或基础设施，确保当Llama 3、4、5、Mixtral、Mistral出现时，它们将帮助你采用它。我可以回到我之前谈到的案例研究。我们以这种方式构建，显然，用Mixtral替换Llama 3非常容易，当Llama 3出现时。例如，如果出现了更好的Embedder，就像几周前出现的非常好的Embedder，我们也可以很容易地替换它。\\n6. GPU看起来真的很贵，无论如何都要使用它们\\nGPU看起来真的很贵。无论如何都要使用它们。GPU是如此惊人。它们非常适合生成式AI和生成式AI工作负载。生成式AI涉及大量并行计算，这恰好是GPU非常擅长的事情。你可能会看价格标签，觉得它比CPU贵100倍。是的，确实如此，但如果你正确使用它并从中获得你需要的利用率，那么最终处理的订单数量将会多得多，而且每个请求的成本将会便宜得多。\\n7. 尽可能用小型模型\\n当你可以的时候，使用小型模型。GPT-4是王者，但你不会让王者洗碗。洗碗是什么：GPT-4是了不起的。它是一项真正卓越的技术，但使它如此出色的是它在能力上非常广泛。我可以使用GPT-4模型写情书，你可以用它成为一个更好的程序员，我们使用的是完全相同的模型。这很疯狂。那个模型有很多能力，因此它真的非常大。它是一个巨大的模型，而且推理起来非常昂贵。我们发现，你最好使用GPT-4来处理那些开源模型还无法处理的真正困难的事情，然后使用较小的模型来处理那些更容易的事情。通过这样做，你可以大幅降低成本和延迟。当我们谈到你之前拥有的延迟预算或资源预算时，如果你只在真正需要的时候使用GPT-4，你可以最大限度地利用资源预算。\\n三个常见的例子是RAG Fusion。这是当你的查询被大型语言模型编辑后，然后所有查询都进行搜索，然后结果进行排名以提高搜索质量。例如，你可以通过不使用GPT-4而获得很好的结果，只在必要时使用GPT-4。例如，使用RAG，你可以只使用一个生成模型来重新排名，所以只是在最后检查Embedder说相关的东西是否真的相关。小型模型，特别是针对函数调用的微调模型非常好。函数调用的一个非常常见的用例是，如果需要我的模型输出类似JSON或regex的东西，我基本上有两种方法可以做到这一点。要么我可以微调一个更小的模型，要么我可以给我的小模型添加控制器。控制器真的很酷。控制器本质上是，如果我自托管模型，我可以禁止我的模型说出任何会破坏JSON模式或我不想要的regex模式的标记。像这样的事情，实际上大多数企业用例，你不一定需要使用那些基于API的模型，你可以立即获得成本和延迟的好处。\\n3 总结\\n确定你的部署边界，然后反向工作。因为你知道你的部署边界，你知道你应该选择的模型，当你将其量化下来时，就是那个大小。花时间思考优化推理，这可以真正地产生多个数量级的差异。生成式AI受益于基础设施的整合，所以尽量避免让每个团队负责他们的部署，因为很可能会出错。假设你将在12个月内替换你的模型进行构建。GPU看起来很贵，但它们是你最好的选择。当你可以的时候，你会使用小型模型。然后我们对Russell说这些，然后他说，“这太有帮助了。我非常兴奋地使用你的提示部署我的关键任务LLM应用。”然后我们说，“没问题，如果你有任何问题，请让我们知道”。\\n4 问答\\nQ：你说过要为灵活性而构建。频繁更换模型的用例是什么？我们在自定义微调和自定义数据上花费的时间和精力将不得不重复？在频繁更换模型的情况下，你有什么建议吗？\\nA：你什么时候想要频繁更换模型？一直都是。随LLM改进速度，几乎总是可以仅通过更换模型就获得更好性能。你可能需要对提示进行一些调整，但通常，一对一的切换是可行的。例如，如果我的应用构建在GPT-3.5上，我将其替换为GPT-4，即使我使用相同的提示，我的模型性能可能会提高，这是一件非常低努力的事情。这与更换所需的工程努力如何协调？如果这是一个月的长过程，如果没有显著改进，那么你就不应该进行那个切换。我建议尝试以一种方式构建，使其不是一个月的长过程，实际上可以在几天内完成，因为那样几乎总是值得的。\\n这与微调如何协调？我有一个辛辣而热门的观点，即对于大多数用例，你不需要微调。微调在几年前的深度学习中非常流行。随模型越来越好，它们也更擅长遵循你的指示。你通常不需要为许多用例进行微调，可用RAG、提示工程和函数调用等方法。这就是我倾向于说的。如果你正在寻找你的第一个LLM用例，谈论更换模型，一个非常好的第一个LLM用例就是尝试替换你的NLP管道。许多企业都有现成的NLP管道。如果你可以将它们替换为LLMs，通常，你会获得多个点的准确性提升。\\nQ：你认为企业级硬件和消费者最大硬件在本地硬件上的区别是什么，因为我选择了消费者最大硬件，因为你的内存可以高达6000兆传输，PCI通道更快。\\nA：因为像他这样的人已经拿走了所有的A100s，当我们进行内部开发时，我们实际上使用的是4090s，这是消费者硬件。它们更容易获得，也比获得数据中心硬件便宜得多。这就是我们用于开发的东西。我们实际上没有使用消费者级硬件进行大规模推理，尽管没有理由它不会工作。\\n如果它适合你的工作负载。我们也使用它。我们认为它们非常好。它们也便宜得多，因为它们作为消费者级而不是数据中心级出售。\\nQ：你说GPU是一个整体，也是最重要的。我有点惊讶，但也许我的问题会解释。我用只有CPU的小虚拟机做了一些概念验证，我每秒几次请求就得到了相当好的结果。我没有问自己关于可扩展性的问题。我在想我们应该在多少请求时切换到GPU？\\nA：实际上，也许我在GPU方面有点过于强烈，因为我们也在CPU上部署过。如果延迟足够好，这通常是人们首先抱怨的问题，是延迟，那么CPU可能没问题。只是当你在寻找规模经济并且当你在寻找扩展时，它们几乎总是每个请求更贵。如果你的请求数量合理地低，延迟也足够好，那么你可以继续使用它。我认为我们的第一个推理服务器的概念验证是在CPU上完成的。你也会知道的另一件事是，你将限制你可以使用的模型的大小。例如，如果你正在做一个70亿量化的，你可能也可以继续使用CPU。我认为如果你从一张白纸开始，GPU更好。如果你的起点是你已经有一个充满CPU的大型数据中心，而且你否则不会使用它们，那么仍然值得尝试是否可以利用它们。\\nQ：我有一个关于通常使用的API的问题，当然，OpenAI的API通常也被应用程序使用。我也知道很多人真的不喜欢OpenAI的API。你看到其他API了吗？因为很多人只是在模仿它们，或者他们只是使用它，但没有人真的喜欢它。\\nA：当你说他们不喜欢它时，是他们不喜欢API结构，还是不喜欢模型？\\nQ：这是关于API结构的。这是关于文档的。这是关于状态的，关于你无法完全理解的很多事情。\\nA：我们也真的不喜欢它，所以我们编写了自己的API，称为我们的推理服务器，然后我们有一个与OpenAI兼容的层，因为大多数人使用那种结构。你可以查看我们的文档，看看你是否更喜欢它。我认为，因为它是第一个真正爆发的，它是整个行业在API结构上汇聚的地方。\\n关注我，紧跟本系列专栏文章，咱们下篇再续！\\n\\n作者简介：魔都架构师，多家大厂后端一线研发经验，在分布式系统设计、数据平台架构和AI应用开发等领域都有丰富实践经验。\\n各大技术社区头部专家博主。具有丰富的引领团队经验，深厚业务架构和解决方案的积累。\\n负责：\\n\\n中央/分销预订系统性能优化\\n活动&券等营销中台建设\\n交易平台及数据中台等架构和开发设计\\n车联网核心平台-物联网连接平台、大数据平台架构设计及优化\\nLLM Agent应用开发\\n区块链应用开发\\n大数据开发挖掘经验\\n推荐系统项目\\n\\n目前主攻市级软件项目设计、构建服务全社会的应用系统。\\n\\n参考：\\n\\n编程严选网\\n\\n\\n本文由博客一文多发平台 OpenWrite 发布！\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.cnblogs.com/JavaEdge/p/18559650'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5c2d11-eff2-43d1-8800-9d9d3a301f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8902\n"
     ]
    }
   ],
   "source": [
    "# 检查加载的文档内容长度\n",
    "print(len(docs[0].page_content))  # 打印第一个文档内容的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94fd046-6a2a-4761-a83b-a9f82ed53b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM部署，你必须要知道的几个技巧！\n",
      "\n",
      "\n",
      "\n",
      "0 前言\n",
      "今天我会首先解释为什么 LLM 的部署很难，因为许多人可能并不理解其中的复杂性。接着，我会分享七个提高 LLM 部署效果的技巧和方法。\n",
      "1 为\n"
     ]
    }
   ],
   "source": [
    "# 查看第一个文档（前100字符）\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26600ceb-0de8-4820-b438-ed8ae561886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6705c5-54ab-4cbd-9718-01563257e54d",
   "metadata": {},
   "source": [
    "### Step 2: 文档分割\n",
    "\n",
    "- **描述**: 使用文本分割器将加载的长文档分割成较小的块，以便嵌入和检索。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `RecursiveCharacterTextSplitter`\n",
    "  - 方法: `split_documents()`\n",
    "- **代码解释**:\n",
    "  - **文档分割**: 使用 `RecursiveCharacterTextSplitter` 按字符大小分割文档块，设置块大小和重叠字符数，确保文档块适合模型处理。\n",
    "  - **检查块数量**: 打印分割后的文档块数量，确保分割操作正确执行。\n",
    "  - **验证块大小**: 输出第一个块的字符数，确认分割块的大小是否符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0958c3b5-4110-44b2-9e27-4a82015c0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 RecursiveCharacterTextSplitter 将文档分割成块，每块1000字符，重叠200字符\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d73830-8e73-4293-80b0-7e631f7344d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# 检查分割后的块数量和内容\n",
    "print(len(all_splits))  # 打印分割后的文档块数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5263f892-4a80-42b9-b79e-ac3e303a60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits[0].page_content))  # 打印第一个块的字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46183c0c-91ec-4609-b7c3-e85c08d8d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM部署，你必须要知道的几个技巧！\n",
      "\n",
      "\n",
      "\n",
      "0 前言\n",
      "今天我会首先解释为什么 LLM 的部署很难，因为许多人可能并不理解其中的复杂性。接着，我会分享七个提高 LLM 部署效果的技巧和方法。\n",
      "1 为啥 LLM 部署困难？\n",
      "“最近在忙啥？”\n",
      "“我一直在让 LLM 服务变得更简单。”\n",
      "“LLM 部署难吗？不是直接调用 OpenAI API 就行？”\n",
      "“某种程度上是这样。”因为提到 LLM，大多数人只会想到 OpenAI，调用 API 确实简单。她为什么要谈这些内容？调用 API 谁不会？但实际上，访问 LLM 的方式不止一种。可用托管的API如 OpenAI、Cohere、Anthropic 和 AI21 Labs 等。他们已为你完成托管和部署，你只需调它们。虽然这确实减少你的工作量，但仍存在复杂性，如减少幻觉输出。不过，他们已经完成很多繁重任务。很多场景，你可能更倾向自托管，如调用 Mistral或托管 Llama 或其他模型。这意味着你在自己的环境中托管它，无论VPC还是PC。\n",
      "那为啥还自托管？\n",
      "很多原因：\n",
      "\n",
      "降低大规模部署成本。如只做概念验证，基于 OpenAI API 模型成本确实低。但如大规模部署，自托管最终成本更低。因为只需解决自己的业务问题，可用更小模型，而 OpenAI 必须托管一个能解决编程和写作莎士比亚问题的大模型，因此需要更大的模型。大规模部署时，自托管成本会低得多\n",
      "性能提升。当你用特定任务的LLM或对其微调，使其专注你的任务，通常得到更好性能\n",
      "大多数客户选择自托管的原因：隐私和安全。如你处受监管行业，如需遵循 GDPR 或满足合规团队的要求，你可能也需自托管\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content)  # 打印第一个块的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034ce612-96d0-4b61-8d53-884b48a85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.cnblogs.com/JavaEdge/p/18559650', 'start_index': 2}\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].metadata)  # 打印第一个块的元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96fb11-41cb-4d79-b7ec-cc294187baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d04622f-7e3c-413c-89cd-5eef87949966",
   "metadata": {},
   "source": [
    "### Step 3: 存储嵌入\n",
    "\n",
    "- **描述**: 将分割后的文档内容嵌入到向量空间中，并存储到向量数据库，以便后续检索。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `Chroma`\n",
    "  - 方法: `from_documents()`\n",
    "  - 类: `OpenAIEmbeddings`\n",
    "- **代码解释**:\n",
    "  - **存储嵌入**: 使用 `Chroma.from_documents()` 方法将所有分割的文档片段进行嵌入(`OpenAIEmbeddings`嵌入模型)，将文档片段嵌入向量空间，并存储在向量数据库中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f1eca-7d0c-4bcc-b255-054cc002d7f4",
   "metadata": {},
   "source": [
    "#### Chroma 基础使用\n",
    "\n",
    "**下面是初始化 Chroma 数据库（仅实例化，未存储向量数据）的常见做法：**\n",
    "\n",
    "**使用构造函数初始化**: 在本地持久化存储 Chroma 数据库.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")\n",
    "```\n",
    "\n",
    "**使用 Cleint 初始化**: 更方便地访问底层数据库/集合。\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "**我们直接使用 `Chroma.from_documents()` 方法 实例化+数据存储**:\n",
    "\n",
    "该方法返回 Chroma 实例，数据类型为`langchain_chroma.vectorstores.Chroma`，详细 API 文档： https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05815c9c-e693-486c-971a-0cc5b756145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16543ab8-fa12-4739-ad58-9e701d260b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: httpx in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx) (4.7.0)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx) (1.0.7)\n",
      "Requirement already satisfied: idna in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /root/anaconda3/envs/peftnew/lib/python3.10/site-packages (from anyio->httpx) (4.12.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef288a05-25a4-4f2f-827a-a5f318ff0d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/testyy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"/data/model/bge-large-zh-v1.5\", model_kwargs=model_kwargs)\n",
    "                        \n",
    "# 使用 Chroma 向量存储和 OpenAIEmbeddings 模型，将分割的文档块嵌入并存储\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39aa3545-b456-473a-b620-5fb0eab15dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 vectorstore 数据类型\n",
    "type(vectorstore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b52313-646a-4512-8125-68054c1c7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c587ce-b40f-47ad-bd14-480c49a073e7",
   "metadata": {},
   "source": [
    "### Step 4: 检索文档\n",
    "\n",
    "- **描述**: 使用 `VectorStoreRetriever` 类的 `as_retriever()` 和 `invoke()` 方法，从向量数据库中检索与查询最相关的文档片段。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `VectorStoreRetriever`\n",
    "  - 方法: `as_retriever()`, `invoke()`\n",
    "- **代码解释**:\n",
    "  - **文档检索**: 将向量存储转换为检索器，并基于查询执行相似性搜索，获取相关文档片段。\n",
    "  - **检查检索数量**: 打印检索到的文档片段数量，确保检索操作成功。\n",
    "  - **验证检索内容**: 输出第一个检索到的文档内容，确认检索结果与预期相符。\n",
    "\n",
    "在 LangChain 中，所有向量数据库都支持**vectorstore.as_retriever** 方法，实例化该数据库对应的检索器（Retriever），数据类型为`VectorStoreRetriever`，详细 API 文档：https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cdab30e-b07d-4273-b734-3aabff356bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 VectorStoreRetriever 从向量存储中检索与查询最相关的文档\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6feddb48-d07e-40be-a9fe-790c3bb75d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.vectorstores.VectorStoreRetriever"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9283b2e6-874f-4815-915b-e87da0429a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"企业选择开源的主要原因?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d292f13-9581-4f28-b54d-ba0fe9fa0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# 检查检索到的文档内容\n",
    "print(len(retrieved_docs))  # 打印检索到的文档数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "458d6a7d-21a9-49a4-b523-651e3bab51e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果这几点不重要，就用 API 够了。\n",
      "企业选择开源的主要原因\n",
      "包括控制权、定制化和成本。最重要的是控制权。拥有 AI 独立性至关重要，如当 OpenAI 再次解雇 CEO，你仍可访问自己的模型，尤其是当你构建重要的业务应用时。如果你正在考虑自托管，你绝对不是孤军奋战，大多数企业都在努力建立自托管能力。\n",
      "对冲基金的一员说：“隐私对我的用例很重要，因此自托管是有意义的。”然后他可能会问：“自托管真的有那么难吗？”我经常听到类似的话，这让我非常恼火。答案是：确实更难。你不能忽视那些你看不到的复杂性。当你调用基于 API 的模型时，你受益于他们的工程师在构建推理和服务基础设施方面所做的所有努力。实际上，像 OpenAI 这样的公司有 50 到 100 人的团队在管理这些基础设施。包括模型压缩、Kubernetes、批处理服务器、函数调用、JSON 生成、运行时引擎等。当你使用 API 模型时，这些你都不需要操心，但当你自托管时，这些问题突然变成了你的责任。\n",
      "他可能会说：“但我经常部署机器学习模型，比如 XGBoost 或线性回归模型。部署这些 LLM 会有多难？”我们的回答是：“你知道 L 代表什么吗？”部署这些模型要困难得多。为什么呢？LLM 中的第一个 L 代表“大”（Large）。我记得我们刚成立公司时，认为一个拥有 1 亿参数的 BERT 模型已经算大了。现在，一个拥有 70 亿参数的模型被认为是小型模型，但它仍然有 14GB 的大小，这绝对不小。\n",
      "第二个原因是 GPU。与 CPU 相比，GPU 更难处理，它们也更昂贵，因此高效利用 GPU 十分重要。如果你对 CPU 的利用率不高，可能问题不大，因为它们成本低得多。但对于 GPU，成本、延迟和性能之间的权衡非常明显，这是以前可能没有遇到过的。\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)  # 打印第一个检索到的文档内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc6302-1b59-4abc-813a-6b0d44cc7cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3309c1e-c650-44ae-baba-6f7528372931",
   "metadata": {},
   "source": [
    "### Step 5: 生成回答\n",
    "\n",
    "- **描述**: 将之前构建的组件（检索器、提示、LLM等）组合成一个完整的链条，实现用户问题的检索与生成回答。完整链条：输入用户问题，检索相关文档，构建提示，将其传递给模型（使用`ChatOpenAI` 类的 `invoke()` 方法），并解析输出生成最终回答。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `ChatOpenAI`\n",
    "  - 方法: `invoke()`\n",
    "  - 类: `RunnablePassthrough`\n",
    "  - 类: `StrOutputParser`\n",
    "  - 模块：`hub`\n",
    "- **代码解释**:\n",
    "  - **模型初始化**: 使用 `ChatOpenAI` 类初始化一个 `GPT-4o-mini` 模型，准备处理生成任务。\n",
    "  - **文档格式化**: 定义 `format_docs` 函数，用于将检索到的文档内容格式化为字符串。\n",
    "  - **构建 RAG 链**: 使用 LCEL (LangChain Execution Layer) 的 `|` 操作符将各个组件连接成一个链条，包括文档检索、提示构建、模型调用以及输出解析。\n",
    "  - **生成回答**: 使用 `stream()` 方法逐步输出生成的回答，并实时展示，确保生成的结果符合预期。\n",
    "\n",
    "![retrieval](../images/retrieval.png)\n",
    "\n",
    "#### LangChain Hub\n",
    "\n",
    "`LangChain Hub` (https://smith.langchain.com/hub) 是一个提示词模板开源社区，为开发者提供了大量开箱即用的提示词模板。属于 `LangSmith` 产品的一部分。\n",
    "\n",
    "下面我们尝试使用 RAG 应用的提示词模板：https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "\n",
    "```\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3dbe88b-7a8f-4607-8426-698a3adf3ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义 RAG 链，将用户问题与检索到的文档结合并生成答案\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "488e8c40-a304-4538-a84e-cc8e4e7e101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/testyy/lib/python3.10/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "# 使用 hub 模块拉取 rag 提示词模板\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf0de4c1-e389-430b-a472-521960e12b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "# 打印模板\n",
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "281d2ea1-0a61-45aa-810d-6b80a7d102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 context 和 question 填充样例数据，并生成 ChatModel 可用的 Messages\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e55e098-5f2d-4ef6-98e2-b1a4f27fa853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# 查看提示词\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cbbc6-aaaa-4f8c-8382-1cbda08d2fe9",
   "metadata": {},
   "source": [
    "#### ⭐️**LCEL 在 RAG 中的应用**⭐️\n",
    "\n",
    "##### **LCEL 概述**\n",
    "\n",
    "LCEL 是 LangChain 中的一个重要概念，它提供了一种统一的接口，允许不同的组件（如 `retriever`, `prompt`, `llm` 等）可以通过统一的 `Runnable` 接口连接起来。每个 `Runnable` 组件都实现了相同的方法，如 `.invoke()`、`.stream()` 或 `.batch()`，这使得它们可以通过 `|` 操作符轻松连接。\n",
    "\n",
    "##### **LCEL 中处理的组件**\n",
    "\n",
    "- **Retriever**: 负责根据用户问题检索相关文档。\n",
    "- **Prompt**: 根据检索到的文档构建提示，供模型生成回答。\n",
    "- **LLM**: 接收提示并生成最终的回答。\n",
    "- **StrOutputParser**: 解析 LLM 的输出，只提取字符串内容，供最终显示。\n",
    "\n",
    "##### **LCEL 运作机制**\n",
    "\n",
    "- **构建链条**: 通过 `|` 操作符，我们可以将多个 `Runnable` 组件连接成一个 `RunnableSequence`。LangChain 会自动将一些对象转换为 `Runnable`，如将 `format_docs` 转换为 `RunnableLambda`，将包含 `\"context\"` 和 `\"question\"` 键的字典转换为 `RunnableParallel`。\n",
    "\n",
    "- **数据流动**: 用户输入的问题会在 `RunnableSequence` 中依次经过各个 `Runnable` 组件。首先，问题会通过 `retriever` 检索相关文档，然后通过 `format_docs` 将这些文档转换为字符串。`RunnablePassthrough` 则直接传递原始问题。最后，这些数据被传递给 `prompt` 来生成完整的提示，供 LLM 使用。\n",
    "\n",
    "##### **LCEL 中的关键操作**\n",
    "\n",
    "- **格式化文档**: `retriever | format_docs` 将问题传递给 `retriever` 生成文档对象，然后通过 `format_docs` 将这些文档格式化为字符串。\n",
    "- **传递问题**: `RunnablePassthrough()` 直接传递原始问题，保持原样。\n",
    "- **构建提示**: `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt` 构建完整的提示。\n",
    "- **运行模型**: `prompt | llm | StrOutputParser()` 运行 LLM 生成回答，并解析输出。\n",
    "\n",
    "#### 使用 LCEL 构建 RAG Chain\n",
    "\n",
    "下面我们将 LCEL 的概念与代码实现结合起来，展示了如何通过一系列 `Runnable` 组件来实现完整的 RAG 流程。通过 LCEL，LangChain 提供了高度模块化和可扩展的开发方式，使复杂任务的实现变得更加简单和高效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe460ec-547e-4714-a7dd-bf9dc319b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义格式化文档的函数\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66e6049b-af4d-435d-aded-68d9c80803bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LCEL 构建 RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2880d17-79ac-4e5c-966b-144d9ee0dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "企业选择开源的主要原因包括控制权、定制化和成本。控制权对于拥有AI独立性至关重要，尤其在构建重要的业务应用时。此外，开源模型的自托管能力让企业可以更灵活地管理基础设施和模型替换周期。"
     ]
    }
   ],
   "source": [
    "# 流式生成回答\n",
    "for chunk in rag_chain.stream(\"企业选择开源的主要原因??\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb6b9b0e-9883-4375-8bc1-ea31f3659172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM部署是指大型语言模型的部署过程。在部署LLM时，需要考虑部署边界、硬件选择、模型替换周期等因素，以提高性能和效率。选择自托管、考虑GPU利用率优化以及量化模型等技巧可以帮助简化LLM部署过程。"
     ]
    }
   ],
   "source": [
    "# 流式生成回答\n",
    "for chunk in rag_chain.stream(\"什么是LLM部署?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a072d01-60b2-46c7-a104-c8a4a3eda0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c3d53d-6ae2-4e05-878b-6702c10995bd",
   "metadata": {},
   "source": [
    "# Homework\n",
    "1. 使用其他的线上文档或离线文件，重新构建向量数据库，尝试提出3个相关问题，测试 LCEL 构建的 RAG Chain 是否能成功召回。\n",
    "2. 重新设计或在 LangChain Hub 上找一个可用的 RAG 提示词模板，测试对比两者的召回率和生成质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91a2d-56e0-4f52-b6a3-dc893583b51f",
   "metadata": {},
   "source": [
    "### 自定义 Prompt 的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94c5d89d-d642-41cb-ab6f-6ead404b6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "filler context\n",
    "\n",
    "用户问：\n",
    "filler question\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "985266bc-1239-45b1-ab76-d7bd2f279cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      "filler context\n",
      "\n",
      "用户问：\n",
      "filler question\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为 context 和 question 填充样例数据，生成 LLM 可用的提示词\n",
    "print(custom_rag_prompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fbe82ee-de48-4cc8-875c-d74242ef273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新自定义 RAG Chain\n",
    "custom_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b818824-5a44-461a-b6d5-3ff079beaaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我无法回答您的问题。'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义 prompt 生成回答\n",
    "custom_rag_chain.invoke(\"什么是LLM部署?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1fa5d-674d-4e1c-a423-303aea90415b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
