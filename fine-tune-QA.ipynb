{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡\n",
    "\n",
    "æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**\n",
    "\n",
    "### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"/data/model/distilbert-base-uncased/\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## ä¸‹è½½æ•°æ®é›†\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚\n",
    "\n",
    "### SQuAD æ•°æ®é›†\n",
    "\n",
    "**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚\n",
    "\n",
    "SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caf6726f58f45dd86e34efc9d1c78c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2173a9beda8492d8d32694c92a0808a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"/data/datasets/squad/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯¹æ¯”æ•°æ®é›†\n",
    "\n",
    "ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š\n",
    "\n",
    "**YelpReviewFull Datasetï¼š**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f4190066117f',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is in front of the Notre Dame Main Building?',\n",
       " 'answers': {'text': ['a copper statue of Christ'], 'answer_start': [188]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5726e1be5951b619008f814a</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>In the last two weeks of February 1951, Operation Roundup was followed by Operation Killer, carried out by the revitalized Eighth Army. It was a full-scale, battlefront-length attack staged for maximum exploitation of firepower to kill as many KPA and PVA troops as possible. Operation Killer concluded with I Corps re-occupying the territory south of the Han River, and IX Corps capturing Hoengseong. On 7 March 1951, the Eighth Army attacked with Operation Ripper, expelling the PVA and the KPA from Seoul on 14 March 1951. This was the city's fourth conquest in a years' time, leaving it a ruin; the 1.5 million pre-war population was down to 200,000, and people were suffering from severe food shortages.</td>\n",
       "      <td>How many times was Seoul captured in a year?</td>\n",
       "      <td>{'text': ['This was the city's fourth conquest'], 'answer_start': [526]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57266952f1498d1400e8ded6</td>\n",
       "      <td>London</td>\n",
       "      <td>London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0Â° longitude, and GMT). Other famous landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square, and The Shard. London is home to numerous museums, galleries, libraries, sporting events and other cultural institutions, including the British Museum, National Gallery, Tate Modern, British Library and 40 West End theatres. The London Underground is the oldest underground railway network in the world.</td>\n",
       "      <td>Are there any bridges located in London?</td>\n",
       "      <td>{'text': ['Tower Bridge'], 'answer_start': [409]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5726a8eef1498d1400e8e668</td>\n",
       "      <td>Madonna_(entertainer)</td>\n",
       "      <td>Beginning in April 1985, Madonna embarked on her first concert tour in North America, The Virgin Tour, with the Beastie Boys as her opening act. She progressed from playing CBGB and the Mudd Club to playing large sporting arenas. At that time she released two more hit singles from the album, \"Angel\" and \"Dress You Up\". In July, Penthouse and Playboy magazines published a number of nude photos of Madonna, taken in New York in 1978. She had posed for the photographs as she needed money at the time, and was paid as little as $25 a session. The publication of the photos caused a media uproar, but Madonna remained \"unapologetic and defiant\". The photographs were ultimately sold for up to $100,000. She referred to these events at the 1985 outdoor Live Aid charity concert, saying that she would not take her jacket off because \"[the media] might hold it against me ten years from now.\"</td>\n",
       "      <td>When was Madonna's first concert tour in North America?</td>\n",
       "      <td>{'text': ['Beginning in April 1985'], 'answer_start': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572eb4c8cb0c0d14000f149a</td>\n",
       "      <td>Seven_Years%27_War</td>\n",
       "      <td>The British Prime Minister, the Duke of Newcastle, was optimistic that the new series of alliances could prevent war from breaking out in Europe. However, a large French force was assembled at Toulon, and the French opened the campaign against the British by an attack on Minorca in the Mediterranean. A British attempt at relief was foiled at the Battle of Minorca, and the island was captured on 28 June (for which Admiral Byng was court-martialed and executed). War between Britain and France had been formally declared on 18 May nearly two years after fighting had broken out in the Ohio Country.</td>\n",
       "      <td>What was the reason that the British Prime Minister thought that ware in Europe could be prevented?</td>\n",
       "      <td>{'text': ['the new series of alliances could prevent war from breaking out in Europe'], 'answer_start': [71]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57266ee0708984140094c5c9</td>\n",
       "      <td>Department_store</td>\n",
       "      <td>Ireland developed a strong middle class, especially in the major cities, by the mid-nineteenth century. They were active patrons of department stores. Delany's New Mart was opened in 1853 in Dublin, Ireland. Unlike others, Delany's had not evolved gradually from a smaller shop on site. Thus it could claim to be the first purpose-built Department Store in the world. The word department store had not been invented at that time and thus it was called the \"Monster House\". The store was completely destroyed in the 1916 Easter Rising, but reopened in 1922.</td>\n",
       "      <td>In what year was the store destroyed?</td>\n",
       "      <td>{'text': ['1916'], 'answer_start': [515]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56cdd08862d2951400fa6898</td>\n",
       "      <td>2008_Sichuan_earthquake</td>\n",
       "      <td>Between 64 and 104 major aftershocks, ranging in magnitude from 4.0 to 6.1, were recorded within 72 hours of the main quake. According to Chinese official counts, \"by 12:00 CST, November 6, 2008 there had been 42,719 total aftershocks, of which 246 ranged from 4.0 MS to 4.9 MS, 34 from 5.0 MS to 5.9 MS, and 8 from 6.0 Ms to 6.4 MS; the strongest aftershock measured 6.4 MS.\" The latest aftershock exceeding M6 occurred on August 5, 2008.</td>\n",
       "      <td>When did the latest magnitude 6 aftershock occur?</td>\n",
       "      <td>{'text': ['on August 5, 2008'], 'answer_start': [421]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>572b36debe1ee31400cb82ac</td>\n",
       "      <td>Empiricism</td>\n",
       "      <td>Hume maintained that all knowledge, even the most basic beliefs about the natural world, cannot be conclusively established by reason. Rather, he maintained, our beliefs are more a result of accumulated habits, developed in response to accumulated sense experiences. Among his many arguments Hume also added another important slant to the debate about scientific method â€” that of the problem of induction. Hume argued that it requires inductive reasoning to arrive at the premises for the principle of inductive reasoning, and therefore the justification for inductive reasoning is a circular argument. Among Hume's conclusions regarding the problem of induction is that there is no certainty that the future will resemble the past. Thus, as a simple instance posed by Hume, we cannot know with certainty by inductive reasoning that the sun will continue to rise in the East, but instead come to expect it to do so because it has repeatedly done so in the past.</td>\n",
       "      <td>What did Hume think our beliefs can't be established solely by?</td>\n",
       "      <td>{'text': ['reason'], 'answer_start': [127]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57277993708984140094deaa</td>\n",
       "      <td>Nonprofit_organization</td>\n",
       "      <td>A charity is a nonprofit organisation that meets stricter criteria regarding its purpose and the method in which it makes decisions and reports its finances. For example, a charity is generally not allowed to pay its Trustees. In England and Wales, charities may be registered with the Charity Commission. In Scotland, the Office of the Scottish Charity Regulator serves the same function. Other organizations which are classified as nonprofit organizations elsewhere, such as trade unions, are subject to separate regulations, and are not regarded as \"charities\" in the technical sense.</td>\n",
       "      <td>What is one of the contraints of an NPO with regards to assets?</td>\n",
       "      <td>{'text': ['generally not allowed to pay its Trustees'], 'answer_start': [184]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>570a7c686d058f1900182e98</td>\n",
       "      <td>Beer</td>\n",
       "      <td>A study published in the Neuropsychopharmacology journal in 2013 revealed the finding that the flavour of beer alone could provoke dopamine activity in the brain of the male participants, who wanted to drink more as a result. The 49 men in the study were subject to positron emission tomography scans, while a computer-controlled device sprayed minute amounts of beer, water and a sports drink onto their tongues. Compared with the taste of the sports drink, the taste of beer significantly increased the participants desire to drink. Test results indicated that the flavour of the beer triggered a dopamine release, even though alcohol content in the spray was insufficient for the purpose of becoming intoxicated.</td>\n",
       "      <td>What hormone can be triggered by the flavor of beer alone in males?</td>\n",
       "      <td>{'text': ['dopamine'], 'answer_start': [131]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572672e8708984140094c677</td>\n",
       "      <td>Dutch_language</td>\n",
       "      <td>Both languages are still largely mutually intelligible, although this relation can in some fields (such as lexicon, spelling and grammar) be asymmetric, as it is easier for Dutch speakers to understand written Afrikaans than it is for Afrikaans speakers to understand written Dutch. Afrikaans is grammatically far less complex than Dutch, and vocabulary items are generally altered in a clearly patterned manner, e.g. vogel becomes voÃ«l (\"bird\") and regen becomes reÃ«n (\"rain\"). In South Africa, the number of students following Dutch at university, is difficult to estimate, since the academic study of Afrikaans inevitably includes the study of Dutch. Elsewhere in the world, the number of people learning Dutch is relatively small.</td>\n",
       "      <td>Who has a more difficult time understanding the other's language: Dutch speakers or Afrikaans speakers?</td>\n",
       "      <td>{'text': ['Afrikaans speakers'], 'answer_start': [235]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## é¢„å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 7076, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvains.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer è¿›é˜¶æ“ä½œ\n",
    "\n",
    "åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºæˆªæ–­çš„ç­–ç•¥\n",
    "\n",
    "- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`\n",
    "- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men ' s basketball team have? [SEP] the men ' s basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla ' s record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla ' s 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey ' s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men ' s basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey ' s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids\n",
    "\n",
    "è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚\n",
    "\n",
    "å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many many\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š\n",
    "\n",
    "- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ\n",
    "- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚\n",
    "\n",
    "ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºå¡«å……çš„ç­–ç•¥\n",
    "\n",
    "- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚\n",
    "- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤\n",
    "\n",
    "è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚\n",
    "\n",
    "é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚\n",
    "\n",
    "å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥\n",
    "    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚\n",
    "    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map çš„è¿›é˜¶ä½¿ç”¨\n",
    "\n",
    "ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š\n",
    "\n",
    "- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚\n",
    "- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚\n",
    "- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜\n",
    "\n",
    "datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5df3f3097b54fa38fccd8c4c7d23b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c929a1833646db9c40c4ca77320bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰\n",
    "\n",
    "è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at /data/model/distilbert-base-uncased/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "model_dir = \"/data/model/distilbert-base-uncased-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰\n",
    "\n",
    "æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰\n",
    "\n",
    "ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU ä½¿ç”¨æƒ…å†µ\n",
    "\n",
    "è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 15:39:57 2023\n",
    "\n",
    "Wed Dec 20 15:39:57 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   67C    P0              67W /  70W |  14617MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     16384      C   /root/miniconda3/bin/python               14612MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 36:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>1.252726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.158939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>1.163634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=1.3115618013003427, metrics={'train_runtime': 2165.8171, 'train_samples_per_second': 122.62, 'train_steps_per_second': 1.917, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3115618013003427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**\n",
    "\n",
    "æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118, 107,  72,  35, 107,  34,  73,  41,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43,  22,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  55,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  37,  31,  43,  54,  42,  35,  43,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,\n",
       "          89, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ\n",
    "\n",
    "æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚\n",
    "\n",
    "åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚\n",
    "\n",
    "é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š\n",
    "- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ\n",
    "- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ\n",
    "- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚\n",
    "\n",
    "ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ\n",
    "1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°\n",
    "1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚\n",
    "1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚\n",
    "1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š\n",
    "\n",
    "- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›\n",
    "- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42409062a0dc4cb5a0d6a34038475ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š\n",
    "\n",
    "ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚\n",
    "\n",
    "å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š\n",
    "- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚\n",
    "- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚\n",
    "- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚\n",
    "- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚\n",
    "- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 16.5709, 'text': 'Denver Broncos'},\n",
       " {'score': 14.644647,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 12.746159, 'text': 'Carolina Panthers'},\n",
       " {'score': 12.266491, 'text': 'Broncos'},\n",
       " {'score': 11.708353, 'text': 'Denver'},\n",
       " {'score': 11.349171,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.429613,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.340239,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.422918,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.232365,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 9.086335,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10'},\n",
       " {'score': 9.000566,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 8.861305,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 8.503361,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.481353, 'text': 'champion Denver Broncos'},\n",
       " {'score': 8.268599,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.952728,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion'},\n",
       " {'score': 7.9272785, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 7.3906612,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl'},\n",
       " {'score': 7.386154, 'text': 'Panthers'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**\n",
    "\n",
    "æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚\n",
    "\n",
    "å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚\n",
    "\n",
    "å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee1be9735b74e36a5ed77c7185a21f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"/data/github/evaluate/metrics/squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 73.95458845789972, 'f1': 83.08437053344042}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeworkï¼šåŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/data/model/distilbert-base-uncased-finetuned-squad-new\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 35:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>1.206281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>1.208986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>1.240402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=0.7557553362984188, metrics={'train_runtime': 2134.2002, 'train_samples_per_second': 124.436, 'train_steps_per_second': 1.945, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.7557553362984188, 'epoch': 3.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trained_trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trained_trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trained_trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  54, 118, 107,  72,  35, 107,  34,  73,  52,  80,  91,\n",
       "         156,  35,  40,  91,  80,  58,  77,  31,  42,  53,  52,  35,  53,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43,  22,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  31,  59,  72,  25,  36,  57,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  37,  31,  43,  54,  53,  35,  54,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,\n",
       "          89, 127,  97,  26,  44,  24,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trained_trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 19.223845, 'text': 'Denver Broncos'},\n",
       " {'score': 16.935009,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 15.203906, 'text': 'Broncos'},\n",
       " {'score': 15.110398, 'text': 'Carolina Panthers'},\n",
       " {'score': 13.756634, 'text': 'Denver'},\n",
       " {'score': 12.915071,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 11.847882,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 11.193373,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 11.146299,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10'},\n",
       " {'score': 10.564078,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 10.336874,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 10.192362,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 9.559047,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.356999,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 9.321689, 'text': 'Carolina Panthers 24â€“10'},\n",
       " {'score': 9.0447035,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 8.979578, 'text': 'champion Denver Broncos'},\n",
       " {'score': 8.959632,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion'},\n",
       " {'score': 8.906224,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title'},\n",
       " {'score': 8.904537,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32bc4344ef344ef92d17baced6c66c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"/data/github/evaluate/metrics/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 75.10879848628193, 'f1': 83.88168726558965}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
